# ğŸ¾ EverMate.AI (v1.0 coming soon)
Your Local AI Pet & Friend Â· Privacy-first Â· Offline whenever possible

[English (Canada)](README.en-CA.md) Â· [ä¸­æ–‡](README.zh-CN.md) Â· [PortuguÃªs (Brasil)](README.pt-BR.md) Â· [FranÃ§ais (Canada)](README.fr-CA.md) Â· [æ—¥æœ¬èª](README.ja-JP.md)

## âœ¨ What it is
EverMate.AI makes **longâ€‘term companionship chats** practical and under your control: runs locally by default, no uploads, and organizes your history with two signature ideas â€” **Threeâ€‘Tier Memory** + **Scalable Local Index**.

## ğŸ± Signature features
### 1) Threeâ€‘Tier Memory (Core â†’ Persona â†’ Vault)
- **Core**: highâ€‘frequency topics + style/response cues (e.g., call you â€œyou/æ‚¨â€, include examples, keep a warm tone). Most questions hit Core first â€” faster and steadier.  
- **Persona**: communication preferences, info density, longâ€‘term interests, doâ€™s & donâ€™ts â€” distilled into â‰¤8 bullets. Uses a local LLM via Ollama when available; otherwise falls back to heuristics.  
- **Vault**: everything else is stored **in chunks on disk** and retrieved on demand â€” no giant doc, just short, precise evidence.  
- **Conflict rule**: if history disagrees with your **current input**, current takes precedence.

### 2) Scalable Local Index (Largeâ€‘scale Â· Local Â· Fast)
- **Chunking**: ~2.8K chars per chunk (tweakable), built for millions of words.  
- **Inverted index**: SQLite tables for `terms/postings/chunks`, WAL mode for robust R/W.  
- **BM25 retrieval**: rank candidate chunks, then lift the **best original sentence** as supporting evidence.  
- **Streaming build & incremental updates**: dragâ€‘andâ€‘drop `.docx/.txt` to build as we read; new chats append per turn and refresh Core/Persona periodically.

## ğŸš€ Quick start
1. `python app.py` in the project directory.  
2. Two paths:  
   - **Import history**: drag `.docx/.txt` on the import screen, click â€œBuild/Rebuild Memoryâ€, then start chatting.  
   - **New friend**: just chat. Each turn (â€œyou ask + AI answersâ€) is appended to the index; Core/Persona refresh after thresholds.  
3. Optional: configure **Ollama** (`OLLAMA_URL`, `OLLAMA_MODEL`) to refine Persona locally.

## ğŸ’¼ How it works (in one line)
Start with **Core** for style & frequent topics â†’ use **Persona** for your taste â†’ pull **Vault** for citations. **Short, relevant context** wins.

## ğŸ§² Import vs. New (same engine)
- **Dragâ€‘andâ€‘drop**: `.docx` parsed via stdlib; `.txt` read as a stream.  
- **New friend**: `append_turn` writes incrementally; default refresh every **20** new chunks.

## ğŸ”§ Tunables
`CHUNK_CHARS`=2800 Â· `CORE_TOP_TERMS`=50 Â· `PERSONA_MAX_BULLETS`=8 Â· `REFRESH_EVERY`=20 Â· `retrieve(query,k)`=4â€“8

## ğŸ›¡ï¸ Privacy & localâ€‘first
Runs and stores data locally; no uploads by default. If you switch to remote models, review your network/compliance posture. Encrypt & back up the `memory` folder.

## â“FAQ (about the signature features)
- Persona hasnâ€™t changed? Likely below the refresh threshold â€” keep chatting or rebuild; you can also lower `REFRESH_EVERY`.  
- Want the **original wording**? Yes â€” Vault returns the most relevant sentence/snippet.  
- Index keeps growing? Increase `CHUNK_CHARS`, archive older `chunks`, or split `memory` per friend.  
- No local LLM? Only Persona quality is affected; heuristics keep the flow stable.  
- Feeling â€œstuckâ€ to history? Current input overrides; you can also tweak `01_core.md` / `02_persona.md` then rebuild.

## ğŸ—ºï¸ Roadmap
Hybrid retrieval (BM25 + local embeddings), topic buckets & timeline, explainability panel, memory decay, event cards.

## ğŸ“¦ Layout
```
memory/
  index.sqlite    # inverted index & stats
  chunks/         # chunked text
  uploads/        # imported files
  buffer.txt      # incremental buffer
  01_core.md      # core memory (freq & style)
  02_persona.md   # persona bullets
  03_vault.md     # vault note (dynamic retrieval)
```

## ğŸ“œ License
MIT (see repository LICENSE)
