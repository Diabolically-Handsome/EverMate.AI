# ğŸ¾ EverMate.AI (v1.0 em breve)
Seu amigo IA local Â· Privacidade em primeiro lugar Â· Offline sempre que possÃ­vel

[English (Canada)](README.en-CA.md) Â· [ä¸­æ–‡](README.zh-CN.md) Â· [PortuguÃªs (Brasil)](README.pt-BR.md) Â· [FranÃ§ais (Canada)](README.fr-CA.md) Â· [æ—¥æœ¬èª](README.ja-JP.md)

## âœ¨ O que Ã©
O EverMate.AI torna **conversas de companhia de longo prazo** prÃ¡ticas e sob seu controle: roda localmente, nÃ£o envia dados por padrÃ£o e organiza tudo com dois diferenciais â€” **MemÃ³ria em TrÃªs Camadas** + **Ãndice Local EscalÃ¡vel**.

## ğŸ± Nossos diferenciais
### 1) MemÃ³ria em TrÃªs Camadas (Core â†’ Persona â†’ Vault)
- **Core**: tÃ³picos frequentes + pistas de estilo/resposta (ex.: tratar por â€œvocÃªâ€, trazer exemplos, tom acolhedor). A maioria das perguntas acerta aqui primeiro â€” mais rÃ¡pido e consistente.  
- **Persona**: preferÃªncias de comunicaÃ§Ã£o, densidade de informaÃ§Ã£o, interesses de longo prazo e â€œnÃ£o me diga issoâ€ â€” em atÃ© 8 bullets. Usa LLM local via Ollama quando disponÃ­vel; caso contrÃ¡rio, usa heurÃ­sticas.  
- **Vault**: o restante fica **fragmentado em arquivos no disco** e Ã© recuperado sob demanda â€” nada de textÃ£o infinito, sÃ³ o trecho certo na hora certa.  
- **Conflitos**: se o histÃ³rico divergir do que vocÃª acabou de dizer, **vale o que acabou de dizer**.

### 2) Ãndice Local EscalÃ¡vel (Grande Â· Local Â· RÃ¡pido)
- **FragmentaÃ§Ã£o**: ~2.8K caracteres por bloco (ajustÃ¡vel), feito para milhÃµes de palavras.  
- **Ãndice invertido**: SQLite com `terms/postings/chunks`, usando WAL para leitura/gravaÃ§Ã£o estÃ¡veis.  
- **BM25**: ranqueia blocos e extrai **a melhor frase original** como evidÃªncia para o contexto.  
- **ConstruÃ§Ã£o contÃ­nua & atualizaÃ§Ãµes incrementais**: arraste `.docx/.txt` e indexamos enquanto lemos; em conversas novas, cada turno Ã© adicionado e Core/Persona se atualizam periodicamente.

## ğŸš€ Guia rÃ¡pido
1. Rode `python app.py`.  
2. Dois caminhos:  
   - **Importar histÃ³rico**: arraste `.docx/.txt`, clique em â€œConstruir/Re-construir MemÃ³riaâ€ e comece a conversar.  
   - **Novo amigo**: sÃ³ conversar. Cada turno entra no Ã­ndice; Core/Persona se atualizam apÃ³s certos marcos.  
3. Opcional: configure **Ollama** (`OLLAMA_URL`, `OLLAMA_MODEL`) para refinar a Persona localmente.

## ğŸ’¼ Como funciona (em uma linha)
ComeÃ§a no **Core** (estilo e temas), passa pela **Persona** (seu jeito), e puxa o **Vault** para citaÃ§Ãµes. Contexto **curto e certeiro**.

## ğŸ§² Importar vs. Criar novo
- **Arrastar & soltar**: `.docx` via biblioteca padrÃ£o; `.txt` em streaming.  
- **Novo amigo**: `append_turn` grava incrementalmente; atualizaÃ§Ã£o padrÃ£o a cada **20** novos blocos.

## ğŸ”§ Ajustes
`CHUNK_CHARS`=2800 Â· `CORE_TOP_TERMS`=50 Â· `PERSONA_MAX_BULLETS`=8 Â· `REFRESH_EVERY`=20 Â· `retrieve(query,k)`=4â€“8

## ğŸ›¡ï¸ Privacidade e local-first
Tudo roda e fica no seu computador. Se usar modelo remoto, revise suas polÃ­ticas. Recomendamos criptografar e fazer backup da pasta `memory`.

## â“FAQ
- A Persona nÃ£o mudou? Provavelmente abaixo do limiar de atualizaÃ§Ã£o â€” continue conversando ou reconstrua; pode reduzir `REFRESH_EVERY`.  
- DÃ¡ para ver a **frase original**? Sim â€” o Vault retorna a sentenÃ§a/trecho mais relevante.  
- O Ã­ndice estÃ¡ crescendo demais? Aumente `CHUNK_CHARS`, arquive `chunks` antigos, ou separe a `memory` por amigo.  
- Sem LLM local? SÃ³ a qualidade da Persona cai; o fluxo segue com heurÃ­sticas.  
- Parece â€œpresoâ€ ao passado? O que vocÃª diz **agora** manda; ajuste `01_core.md` / `02_persona.md` e reconstrua.

## ğŸ—ºï¸ Roadmap
Retrieval hÃ­brido (BM25 + embeddings locais), tÃ³picos/linha do tempo, painel explicÃ¡vel, decaimento de memÃ³ria, cartÃµes de eventos.

## ğŸ“¦ Estrutura
```
memory/
  index.sqlite
  chunks/
  uploads/
  buffer.txt
  01_core.md
  02_persona.md
  03_vault.md
```

## ğŸ“œ LicenÃ§a
MIT (veja LICENSE)
