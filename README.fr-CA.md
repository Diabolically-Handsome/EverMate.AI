# ğŸ¾ EverMate.AI (v1.0 bientÃ´t)
Votre compagnon IA local Â· PrioritÃ© Ã  la vie privÃ©e Â· Hors ligne autant que possible

[English (Canada)](README.en-CA.md) Â· [ä¸­æ–‡](README.zh-CN.md) Â· [PortuguÃªs (Brasil)](README.pt-BR.md) Â· [FranÃ§ais (Canada)](README.fr-CA.md) Â· [æ—¥æœ¬èª](README.ja-JP.md)

## âœ¨ En bref
EverMate.AI rend les **Ã©changes dâ€™accompagnement Ã  long terme** vraiment utilisables et maÃ®trisÃ©s : exÃ©cution locale par dÃ©faut, pas dâ€™envoi, et deux atouts maison â€” **MÃ©moire en trois niveaux** + **Index local extensible**.

## ğŸ± Nos atouts
### 1) MÃ©moire en trois niveaux (Core â†’ Persona â†’ Vault)
- **Core** : thÃ¨mes frÃ©quents + indices de style/rÃ©ponse (ex. ton chaleureux, exemples). La plupart des demandes se rÃ¨glent ici â€” vite et bien.  
- **Persona** : prÃ©fÃ©rences de communication, densitÃ© dâ€™info, intÃ©rÃªts durables, Ã©lÃ©ments Â« Ã  Ã©viter Â» â€” â‰¤8 points. LLM local via Ollama si dispo, sinon heuristiques.  
- **Vault** : le reste est **dÃ©coupÃ© et stockÃ© sur disque**, puis extrait Ã  la demande â€” pas de pavÃ©, seulement la **bonne phrase** au bon moment.  
- **Conflit** : si lâ€™historique contredit votre **entrÃ©e actuelle**, câ€™est lâ€™actuelle qui prime.

### 2) Index local extensible (Grand Â· Local Â· Rapide)
- **DÃ©coupage** : ~2,8 K caractÃ¨res par bloc (ajustable), pensÃ© pour des millions de mots.  
- **Index inversÃ©** : SQLite (`terms/postings/chunks`) en mode WAL pour des E/S stables.  
- **BM25** : classe les blocs et remonte la **meilleure phrase source** comme preuve.  
- **Construction en flux & mises Ã  jour incrÃ©mentales** : glisser-dÃ©poser `.docx/.txt` pour indexer en lecture; les nouvelles conversations sâ€™ajoutent tour par tour, Core/Persona se rafraÃ®chissent pÃ©riodiquement.

## ğŸš€ DÃ©marrage rapide
1. `python app.py` Ã  la racine du projet.  
2. Deux parcours :  
   - **Importer un historique** : glisser-dÃ©poser `.docx/.txt`, cliquer Â« Build/Rebuild Memory Â», puis discuter.  
   - **Nouveau compagnon** : lancez la discussion. Chaque tour sâ€™ajoute Ã  lâ€™index; Core/Persona se mettent Ã  jour au fil de lâ€™eau.  
3. Optionnel : configurer **Ollama** (`OLLAMA_URL`, `OLLAMA_MODEL`) pour affiner la Persona en local.

## ğŸ’¼ Principe (en une phrase)
Dâ€™abord **Core** (style & rÃ©currences), puis **Persona** (vos prÃ©fÃ©rences), et **Vault** pour les citations. Contexte **court et pertinent**.

## ğŸ§² Import vs. Nouveau
- **Glisser-dÃ©poser** : `.docx` via bibliothÃ¨que standard ; `.txt` en streaming.  
- **Nouveau** : `append_turn` Ã©crit en incrÃ©mental ; rafraÃ®chissement par dÃ©faut toutes **20** nouveautÃ©s.

## ğŸ”§ RÃ©glages
`CHUNK_CHARS`=2800 Â· `CORE_TOP_TERMS`=50 Â· `PERSONA_MAX_BULLETS`=8 Â· `REFRESH_EVERY`=20 Â· `retrieve(query,k)`=4â€“8

## ğŸ›¡ï¸ Vie privÃ©e & prioritÃ© au local
Tout reste en local. En cas de modÃ¨le distant, vÃ©rifiez vos exigences lÃ©gales et rÃ©seau. Chiffrez/sauvegardez `memory`.

## â“FAQ
Persona inchangÃ©e ? Seuil de rafraÃ®chissement non atteint â€” continuez ou reconstruisez; baissez `REFRESH_EVERY` au besoin.  
Voir la **phrase dâ€™origine** ? Oui â€” Vault renvoie la phrase/section la plus pertinente.  
Index trop volumineux ? Augmentez `CHUNK_CHARS`, archivez dâ€™anciens `chunks`, ou sÃ©parez `memory` par compagnon.  
Sans LLM local ? Seule la Persona perd en finesse; le flux reste stable.  
Sensation dâ€™Ãªtre Â« figÃ© Â» par lâ€™historique ? Lâ€™entrÃ©e **courante** lâ€™emporte; ajustez `01_core.md` / `02_persona.md` puis reconstruisez.

## ğŸ—ºï¸ Feuille de route
Recherche hybride (BM25 + embeddings locaux), thÃ¨mes & frise, panneau dâ€™explicabilitÃ©, dÃ©cote mÃ©moire, fiches dâ€™Ã©vÃ©nements.

## ğŸ“¦ Arborescence
```
memory/
  index.sqlite
  chunks/
  uploads/
  buffer.txt
  01_core.md
  02_persona.md
  03_vault.md
```

## ğŸ“œ Licence
MIT (voir LICENSE)
